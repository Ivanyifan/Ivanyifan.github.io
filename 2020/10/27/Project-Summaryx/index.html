<!DOCTYPE html>
<html lang="en">
    <!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1">
  
  <title>Project Summary - Yifan Li&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
  
<link rel="stylesheet" href="/css/post.css">

  
<meta name="generator" content="Hexo 5.2.0"></head>
    <body>
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">Blog</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/About-Me">About</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/Ivanyifan?tab=repositories" target="_blank">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        <article class="post">
    <div class="meta">
        
        <div class="date" id="date">
            
            
            
            
            
            
            
            
            
            
            <span>October</span>
            
            
            
            <span>27,</span>
            <span>2020</span>
        </div>
        

        <h2 class="title">Project Summary</h2>
    </div>

    <div class="divider"></div>

    <div class="content">
        <h3 id="What-is-my-project"><a href="#What-is-my-project" class="headerlink" title="What is my project?"></a>What is my project?</h3><p>My project is to using the NLP and deep learning algorithms to develop a method to detect hateful speech on twitter comments in English, direct to women in a group or an individual. </p>
<h3 id="What-are-my-research-target-and-research-method"><a href="#What-are-my-research-target-and-research-method" class="headerlink" title="What are my research target and research method?"></a>What are my research target and research method?</h3><p>My first target of researching is to distinguish the language is hate speech or nor, and after that I need to distinguish it towards to female or immigrants and after that step, I need to find are these speeches towards groups and individual. I mainly adopted RNN (Recurrent Neural Network) as the deep neural networks, to train the data. </p>
<h3 id="What-was-the-biggest-challenge-that-I-encountered"><a href="#What-was-the-biggest-challenge-that-I-encountered" class="headerlink" title="What was the biggest challenge that I encountered?"></a>What was the biggest challenge that I encountered?</h3><p>I need to set up different parameters for the learning rates if the learning rates are too big the tendency and learning will go inversely or over the correct results, and if the learning rate is too small it can’t eliminate the noise, in the data. </p>
<h3 id="How-did-I-overcome-it-What-was-the-most-interesting-part-of-my-project"><a href="#How-did-I-overcome-it-What-was-the-most-interesting-part-of-my-project" class="headerlink" title="How did I overcome it? What was the most interesting part of my project?"></a>How did I overcome it? What was the most interesting part of my project?</h3><p>By the help from my director, Dr. Cheng, I added several more rounds of LSTM (Long short-term memory) into the step, therefore it doesn’t need to train so many times since it cleans the data and avoid the noise. </p>
<h3 id="What-are-my-research-result-s-and-conclusion-s"><a href="#What-are-my-research-result-s-and-conclusion-s" class="headerlink" title="What are my research result(s) and conclusion(s)?"></a>What are my research result(s) and conclusion(s)?</h3><p>The new algorithm has a great progress to the work published in 2019 the correction rate has elevated 30% compare to the old algorithm. And the new developed algorithm’s training time has also decreased the time need for training.</p>
<h3 id="What-have-I-learned-in-this-project-Anything-more-I-want-to-add"><a href="#What-have-I-learned-in-this-project-Anything-more-I-want-to-add" class="headerlink" title="What have I learned in this project? Anything more I want to add?"></a>What have I learned in this project? Anything more I want to add?</h3><p>For the technology part and I learnt the deep training neural networks RNN and CNN, also I become more familiar with colab and py torch， I have also learnt the idea and the willing from my director to improve the technology not is not because to improve my personal life or technology itself, it needs to help other people, and the community. </p>
<h3 id="Coding-Demonstration"><a href="#Coding-Demonstration" class="headerlink" title="Coding Demonstration"></a>Coding Demonstration</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;SemEval2019_Task5_Hateval_Final.ipynb</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Automatically generated by Colaboratory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Original file is located at</span></span><br><span class="line"><span class="string">    https://colab.research.google.com/drive/1450phNa8GjINBcZSTByP4cSKUdPVos6L</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## Import libraries and utility methods</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Extra libraries</span></span><br><span class="line">!pip install revtok <span class="comment"># library to reverse text field in torchtext</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Test google drive</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/content/drive/My Drive/Colab Notebooks/semeval2019/foo.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  f.write(<span class="string">&#x27;Hello Google Drive!&#x27;</span>)</span><br><span class="line">!cat /content/drive/My\ Drive/Colab\ Notebooks/semeval2019/foo.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># built-in libs</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># pytorch stuffs</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># nltk</span></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn</span></span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># colab utils</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line"><span class="comment">#PROJECT_ROOT_PATH = &quot;/content/drive/Shared drives/cmput497_hateval&quot;</span></span><br><span class="line">PROJECT_ROOT_PATH = <span class="string">&quot;/content/drive/My Drive/Colab Notebooks/semeval2019/&quot;</span></span><br><span class="line">LANG = <span class="string">&quot;en&quot;</span></span><br><span class="line">SPACY_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;en&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;path&quot;</span>: os.path.join(PROJECT_ROOT_PATH, <span class="string">&quot;en_core_web_lg-2.2.0/en_core_web_lg/en_core_web_lg-2.2.0&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">CKPT_PATH = &#123;</span><br><span class="line">    <span class="string">&quot;en&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;path&quot;</span>: os.path.join(PROJECT_ROOT_PATH, <span class="string">&quot;checkpoints/en/&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">OUTPUT_PATH = &#123;</span><br><span class="line">    <span class="string">&quot;en&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;path&quot;</span>: os.path.join(PROJECT_ROOT_PATH, <span class="string">&quot;output/en/&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">DEBUG = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mount gdrive</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(PROJECT_ROOT_PATH):</span><br><span class="line">    drive.mount(<span class="string">&quot;/content/drive&quot;</span>, force_remount=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create output path</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(OUTPUT_PATH[LANG][<span class="string">&quot;path&quot;</span>]):</span><br><span class="line">    os.makedirs(OUTPUT_PATH[LANG][<span class="string">&quot;path&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/13896056/how-to-remove-user-mentions-and-urls-in-a-tweet-string-using-python</span></span><br><span class="line">    <span class="comment"># step 1 &amp; 3: remove mentions and url</span></span><br><span class="line">    clean_row = re.sub(<span class="string">r&quot;(?:\@|https?\://)\S+&quot;</span>, <span class="string">&quot;&quot;</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># step 5: extracting words from hashtags using pascal case</span></span><br><span class="line">    <span class="comment"># regex expression: https://stackoverflow.com/questions/1128305/regex-for-pascalcased-words-aka-camelcased-with-leading-uppercase-letter - Nicolas Henneaux solution</span></span><br><span class="line">    pascal = re.findall(<span class="string">r&quot;[A-Z][a-z0-9]*[A-Z0-9][a-z0-9]+[A-Za-z0-9]*&quot;</span>, clean_row)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> pascal:</span><br><span class="line">        <span class="comment"># p_text = p.replace(&#x27;#&#x27;,&#x27;&#x27;)</span></span><br><span class="line">        re_content = re.findall(<span class="string">&quot;[A-Z][^A-Z]*&quot;</span>, p)</span><br><span class="line">        extracted = <span class="string">&quot; &quot;</span>.join(re_content)</span><br><span class="line">        clean_row = re.sub(<span class="string">&quot;#&quot;</span> + p, extracted, clean_row)</span><br><span class="line">    <span class="comment"># step 2: remove punctuation + non-alphanumericals</span></span><br><span class="line">    final_clean = re.sub(<span class="string">r&quot;[^A-Za-z0-9\&#x27;]+&quot;</span>, <span class="string">&quot; &quot;</span>, clean_row)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># step 4: Contracting whitespace</span></span><br><span class="line">    tokens = word_tokenize(<span class="built_in">str</span>(final_clean.strip().lower()))</span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line">nltk.download(<span class="string">&#x27;punkt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Python version:&#x27;</span>, sys.version)</span><br><span class="line">print(<span class="string">&quot;NLTK version &#123;&#125;&quot;</span>.<span class="built_in">format</span>(nltk.__version__))</span><br><span class="line">print(<span class="string">&quot;PyTorch version &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.__version__))</span><br><span class="line">print(<span class="string">&#x27;Torch Text version:&#x27;</span>, torchtext.__version__)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;## Load data&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Data field</span></span><br><span class="line">ID = torchtext.data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># TEXT.reverse() to get original text from encoded text</span></span><br><span class="line">TEXT = torchtext.data.ReversibleField(sequential=<span class="literal">True</span>, tokenize=tokenizer, include_lengths=<span class="literal">True</span>, lower=<span class="literal">True</span>)</span><br><span class="line">HS_LABEL = torchtext.data.Field(</span><br><span class="line">    sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>, is_target=<span class="literal">True</span>, pad_token=<span class="literal">None</span>, unk_token=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">TR_LABEL = torchtext.data.Field(</span><br><span class="line">    sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>, is_target=<span class="literal">True</span>, pad_token=<span class="literal">None</span>, unk_token=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">AG_LABEL = torchtext.data.Field(</span><br><span class="line">    sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>, is_target=<span class="literal">True</span>, pad_token=<span class="literal">None</span>, unk_token=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchGenerator</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dl, x_var, y_vars</span>):</span></span><br><span class="line">        self.dl, self.x_var, self.y_vars = (</span><br><span class="line">            dl,</span><br><span class="line">            x_var,</span><br><span class="line">            y_vars,</span><br><span class="line">        ) </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</span><br><span class="line">            x = <span class="built_in">getattr</span>(batch, self.x_var) </span><br><span class="line">            <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">                y = torch.cat(</span><br><span class="line">                    [<span class="built_in">getattr</span>(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars], dim=<span class="number">1</span></span><br><span class="line">                ).<span class="built_in">float</span>()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = torch.zeros((<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> (x[<span class="number">0</span>], y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">lang, device, config, label, use_pretrained=<span class="literal">False</span></span>):</span></span><br><span class="line">    train_data, validation_data, test_data = torchtext.data.TabularDataset.splits(</span><br><span class="line">        path=<span class="string">&quot;&#123;&#125;/dataset/&quot;</span>.<span class="built_in">format</span>(PROJECT_ROOT_PATH),</span><br><span class="line">        train=<span class="string">&quot;hs-&#123;&#125;.tsv.train&quot;</span>.<span class="built_in">format</span>(lang),</span><br><span class="line">        validation=<span class="string">&quot;hs-&#123;&#125;.tsv.dev&quot;</span>.<span class="built_in">format</span>(lang),</span><br><span class="line">        test=<span class="string">&quot;hs-&#123;&#125;.tsv.test&quot;</span>.<span class="built_in">format</span>(lang),</span><br><span class="line">        <span class="built_in">format</span>=<span class="string">&quot;tsv&quot;</span>,</span><br><span class="line">        skip_header=<span class="literal">True</span>,</span><br><span class="line">        fields=[</span><br><span class="line">            (<span class="string">&quot;id&quot;</span>, ID),</span><br><span class="line">            (<span class="string">&quot;text&quot;</span>, TEXT),</span><br><span class="line">            (<span class="string">&quot;HS&quot;</span>, HS_LABEL),</span><br><span class="line">            (<span class="string">&quot;TR&quot;</span>, TR_LABEL),</span><br><span class="line">            (<span class="string">&quot;AG&quot;</span>, AG_LABEL),</span><br><span class="line">        ],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;Data Summary: Train: &#123;&#125;, Validation: &#123;&#125;, Test: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_data), <span class="built_in">len</span>(validation_data), <span class="built_in">len</span>(test_data)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_pretrained:</span><br><span class="line">        TEXT.build_vocab(train_data, vectors=<span class="string">&quot;glove.twitter.27B.200d&quot;</span>, vectors_cache=os.path.join(PROJECT_ROOT_PATH, <span class="string">&quot;.vector_cache/&quot;</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        TEXT.build_vocab(train_data)</span><br><span class="line">    HS_LABEL.build_vocab(train_data)</span><br><span class="line">    AG_LABEL.build_vocab(train_data)</span><br><span class="line">    TR_LABEL.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line">    train_dataloader, validation_dataloader = torchtext.data.BucketIterator.splits(</span><br><span class="line">        (train_data, validation_data),</span><br><span class="line">        batch_sizes=(config.batch_size, config.batch_size),</span><br><span class="line">        device=device,</span><br><span class="line">        sort_key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.text),</span><br><span class="line">        sort_within_batch=<span class="literal">False</span>,</span><br><span class="line">        repeat=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    test_dataloader = torchtext.data.Iterator(test_data, batch_size=config.test_batch_size, device=device, sort=<span class="literal">False</span>, sort_within_batch=<span class="literal">False</span>, repeat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    train_iter = BatchGenerator(train_dataloader, <span class="string">&quot;text&quot;</span>, label)</span><br><span class="line">    validation_iter = BatchGenerator(validation_dataloader, <span class="string">&quot;text&quot;</span>, label)</span><br><span class="line">    test_iter = BatchGenerator(test_dataloader, <span class="string">&quot;text&quot;</span>, label)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_iter, validation_iter, test_iter</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;## Hyperparameters&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HyperParameters</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.learning_rate = <span class="number">0.0005</span></span><br><span class="line">        self.num_epochs = <span class="number">100</span></span><br><span class="line">        self.momentum = <span class="number">0</span></span><br><span class="line">        self.batch_size = <span class="number">16</span></span><br><span class="line">        self.test_batch_size = <span class="number">32</span></span><br><span class="line">        self.embedding_dim = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;## NN Models&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, batch_size, label=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        print(<span class="string">&quot;Num embedding: &#123;&#125;, Embedding dim: &#123;&#125;, Batch size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(num_embeddings, embedding_dim, batch_size))</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.embed = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, <span class="number">64</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> label != <span class="literal">None</span>:</span><br><span class="line">          self.set_initial_weights(label)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># if pretrained_weights:</span></span><br><span class="line">        <span class="comment">#     self.emb.weight.data.copy_(pretrained_vec)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_initial_weights</span>(<span class="params">self, label</span>):</span></span><br><span class="line">        <span class="keyword">if</span> label[<span class="number">0</span>] == <span class="string">&quot;HS&quot;</span>:</span><br><span class="line">          vocab = HS_LABEL.vocab</span><br><span class="line">        <span class="keyword">elif</span> label[<span class="number">0</span>] == <span class="string">&quot;TR&quot;</span>:</span><br><span class="line">          vocab = TR_LABEL.vocab</span><br><span class="line">        <span class="keyword">elif</span> label[<span class="number">0</span>] == <span class="string">&quot;AG&quot;</span>:</span><br><span class="line">          vocab = AG_LABEL.vocab</span><br><span class="line"></span><br><span class="line">        tensor = torch.tensor((<span class="number">1</span>,<span class="number">64</span>), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        weights = nn.Parameter(torch.cat((tensor.new_full((<span class="number">1</span>,<span class="number">64</span>), <span class="number">1</span>/vocab.freqs[<span class="string">&quot;0&quot;</span>]), tensor.new_full((<span class="number">1</span>,<span class="number">64</span>), <span class="number">1</span>/vocab.freqs[<span class="string">&quot;1&quot;</span>])), <span class="number">0</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">assert</span> weights.shape == self.fc.weight.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            self.fc.weight = weights</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.embed(x)</span><br><span class="line">        x, (h_n, _) = self.lstm(x)</span><br><span class="line">        <span class="comment"># get the hidden state of the last layer</span></span><br><span class="line">        x = self.fc(h_n[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NetGlove</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, batch_size, pretrained_weights=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NetGlove, self).__init__()</span><br><span class="line">        print(<span class="string">&quot;Num embedding: &#123;&#125;, Embedding dim: &#123;&#125;, Batch size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(num_embeddings, embedding_dim, batch_size))</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.embed = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, <span class="number">64</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>, bidirectional=<span class="literal">True</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> pretrained_weights <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embed.weight.data.copy_(pretrained_weights)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_initial_weights</span>(<span class="params">self, label</span>):</span></span><br><span class="line">        <span class="keyword">if</span> label[<span class="number">0</span>] == <span class="string">&quot;HS&quot;</span>:</span><br><span class="line">          vocab = HS_LABEL.vocab</span><br><span class="line">        <span class="keyword">elif</span> label[<span class="number">0</span>] == <span class="string">&quot;TR&quot;</span>:</span><br><span class="line">          vocab = TR_LABEL.vocab</span><br><span class="line">        <span class="keyword">elif</span> label[<span class="number">0</span>] == <span class="string">&quot;AG&quot;</span>:</span><br><span class="line">          vocab = AG_LABEL.vocab</span><br><span class="line"></span><br><span class="line">        tensor = torch.tensor((<span class="number">1</span>,<span class="number">64</span>), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        weights = nn.Parameter(torch.cat((tensor.new_full((<span class="number">1</span>,<span class="number">64</span>), <span class="number">1</span>/vocab.freqs[<span class="string">&quot;0&quot;</span>]), tensor.new_full((<span class="number">1</span>,<span class="number">64</span>), <span class="number">1</span>/vocab.freqs[<span class="string">&quot;1&quot;</span>])), <span class="number">0</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">assert</span> weights.shape == self.fc.weight.shape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            self.fc.weight = weights</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.embed(x)</span><br><span class="line">        x, (h_n, _) = self.lstm(x)</span><br><span class="line">        <span class="comment"># get the hidden state of the last layer</span></span><br><span class="line">        x = self.fc(h_n[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;## Training and stuffs&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, criterion, optimizer, train_data, validation_data, vocabs, device, config, params, last_epoch=<span class="number">1</span>, last_max_valid_acc=<span class="number">0</span>, last_min_valid_loss=<span class="number">1000</span></span>):</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    train_correct = <span class="number">0</span></span><br><span class="line">    train_total = <span class="number">0</span></span><br><span class="line">    max_valid_acc = last_max_valid_acc</span><br><span class="line">    min_valid_loss = last_min_valid_loss</span><br><span class="line"></span><br><span class="line">    total_train_acc = <span class="number">0</span></span><br><span class="line">    total_valid_acc = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(last_epoch, config.num_epochs + <span class="number">1</span>):</span><br><span class="line">        running_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> text, target <span class="keyword">in</span> train_data:</span><br><span class="line">            text = text.to(device) </span><br><span class="line">            target = torch.tensor([t[<span class="number">0</span>] <span class="keyword">for</span> t <span class="keyword">in</span> target.tolist()], dtype=torch.long).to(device)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(text)</span><br><span class="line">            loss = criterion(outputs, target)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            train_correct += pred.eq(target.data.view_as(pred)).cpu().<span class="built_in">sum</span>()</span><br><span class="line">            train_total += target.size(<span class="number">0</span>)</span><br><span class="line">            running_loss += loss.item()</span><br><span class="line">        epoch_loss = running_loss / <span class="built_in">len</span>(train_data)</span><br><span class="line">        train_accuracy = train_correct.data.cpu().numpy() / train_total</span><br><span class="line">        total_train_acc += train_accuracy</span><br><span class="line">        <span class="comment"># validation after each epoch</span></span><br><span class="line">        validation_accuracy, validation_loss, _ = <span class="built_in">eval</span>(model, criterion, validation_data, device)</span><br><span class="line">        total_valid_acc += validation_accuracy</span><br><span class="line">        print(<span class="string">&quot;Epoch: &#123;&#125;, Training Loss: &#123;:.4f&#125;, Validation Loss: &#123;:.4f&#125;, Train Acc: &#123;:.3f&#125;%, Validation Acc: &#123;:.3f&#125;%&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            epoch, </span><br><span class="line">            epoch_loss, </span><br><span class="line">            validation_loss,</span><br><span class="line">            train_accuracy * <span class="number">100</span>,</span><br><span class="line">            validation_accuracy * <span class="number">100</span></span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> validation_accuracy &gt; max_valid_acc:</span><br><span class="line">            model_dict = &#123;</span><br><span class="line">                <span class="string">&#x27;model&#x27;</span>: model.state_dict(),</span><br><span class="line">                <span class="string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">                <span class="string">&#x27;train_loss&#x27;</span>: epoch_loss,</span><br><span class="line">                <span class="string">&#x27;train_acc&#x27;</span>: train_accuracy,</span><br><span class="line">                <span class="string">&#x27;valid_loss&#x27;</span>: validation_loss,</span><br><span class="line">                <span class="string">&#x27;valid_acc&#x27;</span>: validation_accuracy,</span><br><span class="line">                <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">                <span class="string">&#x27;timestamp&#x27;</span>: datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>),</span><br><span class="line">            &#125;</span><br><span class="line">            save_ckpt(model_dict, os.path.join(CKPT_PATH[LANG][<span class="string">&quot;path&quot;</span>], params.ckpt_dir), <span class="string">&quot;model.pt.&#123;:d&#125;&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">            max_valid_acc = validation_accuracy</span><br><span class="line">        model.train()</span><br><span class="line">    print(<span class="string">f&quot;Total epoch: <span class="subst">&#123;config.num_epochs&#125;</span>, Avg Train Acc: <span class="subst">&#123;(total_train_acc / config.num_epochs * <span class="number">100</span>):<span class="number">.3</span>f&#125;</span>%, Avg Valid Acc: <span class="subst">&#123;(total_valid_acc / config.num_epochs * <span class="number">100</span>):<span class="number">.3</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">model, criterion, validation_data, device</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    validation_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    gold_labels = []</span><br><span class="line">    pred_labels = []</span><br><span class="line">    ids = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> validation_data:</span><br><span class="line">            data = data.to(device)</span><br><span class="line">            ids += [t[<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> target.tolist()]</span><br><span class="line">            target = torch.tensor([t[<span class="number">0</span>] <span class="keyword">for</span> t <span class="keyword">in</span> target.tolist()], dtype=torch.long).to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            validation_loss += criterion(output, target).item()</span><br><span class="line">            pred = output.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct += pred.eq(target.data.view_as(pred)).cpu().<span class="built_in">sum</span>()</span><br><span class="line">            total += target.size(<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># save labels</span></span><br><span class="line">            gold_labels += <span class="built_in">list</span>(target.<span class="built_in">int</span>())</span><br><span class="line">            pred_labels += <span class="built_in">list</span>(pred.data.<span class="built_in">int</span>().cpu())</span><br><span class="line">    validation_loss /= <span class="built_in">len</span>(validation_data)</span><br><span class="line">    <span class="keyword">return</span> correct.data.cpu().numpy() / total, validation_loss, (gold_labels, pred_labels, ids)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_ckpt</span>(<span class="params">ckpt, ckpt_dir, ckpt_name</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(ckpt_dir):</span><br><span class="line">        os.makedirs(ckpt_dir)</span><br><span class="line">    torch.save(ckpt, os.path.join(ckpt_dir, ckpt_name))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_best_model</span>(<span class="params">device, params</span>):</span></span><br><span class="line">    weights_dir = os.path.join(CKPT_PATH[LANG][<span class="string">&quot;path&quot;</span>], params.ckpt_dir)</span><br><span class="line">    matching_ckpts = [k <span class="keyword">for</span> k <span class="keyword">in</span> os.listdir(weights_dir) <span class="keyword">if</span></span><br><span class="line">                      os.path.isfile(os.path.join(weights_dir, k))]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> matching_ckpts:</span><br><span class="line">        msg = <span class="string">&quot;No checkpoints found in &#123;&#125;&quot;</span>.<span class="built_in">format</span>(weights_dir)</span><br><span class="line">        <span class="keyword">if</span> params.load_ckpt == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> IOError(msg)</span><br><span class="line">        print(msg)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">       matching_ckpts.sort(key=<span class="keyword">lambda</span> x: [<span class="built_in">int</span>(c) <span class="keyword">if</span> c.isdigit() <span class="keyword">else</span> c <span class="keyword">for</span> c <span class="keyword">in</span> re.split(<span class="string">r&#x27;(\d+)&#x27;</span>, x)])</span><br><span class="line">       ckpt_path = os.path.join(weights_dir, matching_ckpts[<span class="number">-1</span>])</span><br><span class="line">       print(<span class="string">&quot;Loading checkpoint from: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(ckpt_path))</span><br><span class="line">       <span class="keyword">return</span> torch.load(ckpt_path, map_location=device)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainParameters</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :ivar load_ckpt</span></span><br><span class="line"><span class="string">            0: train from scratch</span></span><br><span class="line"><span class="string">            1: load and test</span></span><br><span class="line"><span class="string">            2: load if exists and continue training</span></span><br><span class="line"><span class="string">        :ivar task</span></span><br><span class="line"><span class="string">            0: HS</span></span><br><span class="line"><span class="string">            1: TR</span></span><br><span class="line"><span class="string">            2: AG</span></span><br><span class="line"><span class="string">        :ivar use_pretrained</span></span><br><span class="line"><span class="string">            True: use pre-trained glove model glove.twitter.27b.200d</span></span><br><span class="line"><span class="string">            False: do not use any pre-trained model</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.load_ckpt = <span class="number">0</span></span><br><span class="line">        self.task = <span class="number">0</span></span><br><span class="line">        self.use_pretrained = <span class="literal">True</span> <span class="comment">#True</span></span><br><span class="line">        self.use_balanced_weight = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.task == <span class="number">0</span>:</span><br><span class="line">            self.label = [<span class="string">&quot;HS&quot;</span>, <span class="string">&quot;id&quot;</span>]</span><br><span class="line">            self.output_name = <span class="string">&quot;en_a.tsv&quot;</span></span><br><span class="line">            self.ckpt_dir = <span class="string">&quot;task_1/&quot;</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">elif</span> self.task == <span class="number">1</span>:</span><br><span class="line">            self.label = [<span class="string">&quot;TR&quot;</span>, <span class="string">&quot;id&quot;</span>]</span><br><span class="line">            self.output_name = <span class="string">&quot;en_b.tsv&quot;</span></span><br><span class="line">            self.ckpt_dir = <span class="string">&quot;task_2/&quot;</span></span><br><span class="line">     </span><br><span class="line">        <span class="keyword">elif</span> self.task == <span class="number">2</span>:</span><br><span class="line">            self.label = [<span class="string">&quot;AG&quot;</span>, <span class="string">&quot;id&quot;</span>]</span><br><span class="line">            self.output_name = <span class="string">&quot;en_c.tsv&quot;</span></span><br><span class="line">            self.ckpt_dir = <span class="string">&quot;task_3/&quot;</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;CUDA Available: &quot;</span>,torch.cuda.is_available())</span><br><span class="line">print(<span class="string">&quot;GPU Info: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.cuda.get_device_name()))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line">config = HyperParameters()</span><br><span class="line"><span class="comment"># training params</span></span><br><span class="line">params = TrainParameters()</span><br><span class="line"></span><br><span class="line"><span class="comment"># load data into dataloader</span></span><br><span class="line">train_data, validation_data, test_data = load_data(lang=LANG, device=device, config=config, label=params.label, use_pretrained=params.use_pretrained)</span><br><span class="line">print(<span class="built_in">len</span>(TEXT.vocab))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> params.use_pretrained <span class="keyword">and</span> <span class="keyword">not</span> params.use_balanced_weight:</span><br><span class="line">    <span class="comment"># only pre-trained weight</span></span><br><span class="line">    config.embedding_dim = <span class="number">200</span></span><br><span class="line">    model = NetGlove(<span class="built_in">len</span>(TEXT.vocab), config.embedding_dim, config.batch_size, pretrained_weights=TEXT.vocab.vectors).to(device)</span><br><span class="line"><span class="keyword">elif</span> params.use_balanced_weight <span class="keyword">and</span> <span class="keyword">not</span> params.use_pretrained:</span><br><span class="line">    <span class="comment"># only balanced weight</span></span><br><span class="line">    config.embedding_dim = <span class="number">128</span></span><br><span class="line">    model = Net(<span class="built_in">len</span>(TEXT.vocab), config.embedding_dim, config.batch_size, label=params.label).to(device)</span><br><span class="line"><span class="keyword">elif</span> <span class="keyword">not</span> params.use_balanced_weight <span class="keyword">and</span> <span class="keyword">not</span> params.use_pretrained:</span><br><span class="line">    <span class="comment"># none</span></span><br><span class="line">    config.embedding_dim = <span class="number">128</span></span><br><span class="line">    model = Net(<span class="built_in">len</span>(TEXT.vocab), config.embedding_dim, config.batch_size, label=params.label).to(device)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">&quot;Not supported!&quot;</span>)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Train Config Summay&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;Use pre-trained weight: <span class="subst">&#123;params.use_pretrained&#125;</span>, Use balanced weight: <span class="subst">&#123;params.use_balanced_weight&#125;</span>, Task: <span class="subst">&#123;params.task&#125;</span>, Load ckpt: <span class="subst">&#123;params.load_ckpt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load last checkpoint</span></span><br><span class="line"><span class="keyword">if</span> params.load_ckpt == <span class="number">1</span> <span class="keyword">or</span> params.load_ckpt == <span class="number">2</span>:</span><br><span class="line">    ckpt = load_best_model(device, params)</span><br><span class="line">    <span class="keyword">if</span> ckpt:</span><br><span class="line">        model.load_state_dict(ckpt[<span class="string">&quot;model&quot;</span>])</span><br><span class="line">        optimizer.load_state_dict(ckpt[<span class="string">&quot;optimizer&quot;</span>])</span><br><span class="line">        last_epoch = <span class="built_in">int</span>(ckpt[<span class="string">&quot;epoch&quot;</span>])</span><br><span class="line">        max_valid_acc = ckpt[<span class="string">&quot;valid_acc&quot;</span>]</span><br><span class="line">        min_valid_loss = ckpt[<span class="string">&quot;valid_loss&quot;</span>]</span><br><span class="line">        print(max_valid_acc)</span><br><span class="line">        print(min_valid_loss)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        last_epoch = <span class="number">0</span></span><br><span class="line">        max_valid_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> params.load_ckpt == <span class="number">2</span>:</span><br><span class="line">    <span class="comment"># continue training</span></span><br><span class="line">    train(model, criterion, optimizer, train_data, validation_data, TEXT.vocab, device, config, params, last_epoch=last_epoch+<span class="number">1</span>, last_max_valid_acc=max_valid_acc, last_min_valid_loss=min_valid_loss)</span><br><span class="line"><span class="keyword">elif</span> params.load_ckpt == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># start from stratch</span></span><br><span class="line">    train(model, criterion, optimizer, train_data, validation_data, TEXT.vocab, device, config, params)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Testing...&quot;</span>)</span><br><span class="line">start_t = time.time()</span><br><span class="line">test_accuracy, test_loss, (gold_labels, pred_labels, ids) = <span class="built_in">eval</span>(model, criterion, test_data, device)</span><br><span class="line">end_t = time.time()</span><br><span class="line">test_time = end_t - start_t</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert tensor to regular python datatype</span></span><br><span class="line">ids = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, ids))</span><br><span class="line">gold_labels = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.item(), gold_labels))</span><br><span class="line">pred_labels = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.item(), pred_labels))</span><br><span class="line"></span><br><span class="line"><span class="comment"># save output to tsv</span></span><br><span class="line">tsv_path = os.path.join(OUTPUT_PATH[LANG][<span class="string">&quot;path&quot;</span>], params.output_name)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(tsv_path, <span class="string">&quot;wt&quot;</span>) <span class="keyword">as</span> output_file:</span><br><span class="line">    tsv_writer = csv.writer(output_file, delimiter=<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">    tsv_writer.writerows(<span class="built_in">zip</span>(ids, pred_labels))</span><br><span class="line"></span><br><span class="line"><span class="comment"># f1_score = sklearn.metrics.f1_score(gold_labels, pred_labels)</span></span><br><span class="line">precision, recall, f1_score, _ = sklearn.metrics.precision_recall_fscore_support(gold_labels, pred_labels, average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;Test Loss: <span class="subst">&#123;test_loss:<span class="number">.6</span>f&#125;</span>, Test Acc: <span class="subst">&#123;test_accuracy*<span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%, Test Time: <span class="subst">&#123;test_time:<span class="number">.3</span>f&#125;</span> sec&#x27;</span>)</span><br><span class="line">print(<span class="string">f&quot;F1 Score: <span class="subst">&#123;f1_score&#125;</span>, Precision: <span class="subst">&#123;precision:<span class="number">.3</span>f&#125;</span>, Recall: <span class="subst">&#123;recall:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>



    </div>

    <div class="about">
        <h1>About this Post</h1>
        <p>This post is written by Ivan Lee, licensed under <a
                target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
    </div>
</article>
        <footer>
    <div class="inner">
        <div class="links">
            
            <div class="group">
                <h4 class="title">Blog</h4>
                
                <a href="/" class="item" target="_blank">Blog</a>
                
                <a href="/archives" class="item" target="_blank">Archives</a>
                
                <a href="/About-Me" class="item" target="_blank">About</a>
                
            </div>
            
            <div class="group">
                <h4 class="title">Me</h4>
                
                <a href="https://github.com/Ivanyifan?tab=repositories" class="item" target="_blank">GitHub</a>
                
            </div>
            
        </div>
        &copy; 2020 Ivan Lee<br />
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
</footer>

        
<script src="/js/main.js"></script>

    </body>
</html>
